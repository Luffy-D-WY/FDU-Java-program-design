{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:36:24.362171Z",
     "start_time": "2024-06-11T12:36:21.150490Z"
    }
   },
   "source": [
    "import model.ChainCRF\n",
    "import importlib\n",
    "import dataLoad.lload\n",
    "\n",
    "importlib.reload(model.ChainCRF)\n",
    "importlib.reload(dataLoad.lload)\n",
    "from dataLoad.lload import CustomDataset\n",
    "from dataLoad.lload import *\n",
    "import numpy as np\n",
    "from model.ChainCRF import BLITM, ChainCRF\n",
    "import torchtext\n",
    "from get_data import *\n",
    "from check import *\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "language = \"Chinese\"\n",
    "mode = False\n",
    "hidden_dim = 100 if language == \"English\" else 100\n",
    "embed_size = 100 if language == \"English\" else 200\n",
    "for_test = True\n",
    "min_freq = 4 if language == \"English\" else 4\n",
    "##Chinese 300 100 10,1e-3  Loss:281.3659775416056 micro avg  0.9368 \n",
    "##Chinese 100 100 10,1e-3  Loss:281.3659775416056 micro avg  0.9447 \n",
    "##Chinese 50 100 10,1e-4  Loss:281.3659775416056 micro avg 0.6839 \n",
    "##Chinese 50 100 10,5e-4  Loss:277.4861424763997 micro avg 0.9065\n",
    "## 0.7683 \n",
    "## English\n",
    "##  pretrain 8317 +0.8276 =0.8320  1e-3 10\n",
    "##  pretrain 8317 +0.8276 =0.8347  1e-3 10 zeros\n",
    "##  pretrain 8317 +0.8276 =0.8399  1e-4 10 zeros\n",
    "#   0.8381 \n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:36:24.378062Z",
     "start_time": "2024-06-11T12:36:24.362171Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, num_classes, vocab_length, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.bilstm = BLITM(num_classes, vocab_length, embedding_dim, hidden_dim)\n",
    "        self.crf = ChainCRF(num_classes)\n",
    "\n",
    "    def forward(self, sentence, mask, targets=None, pre_train=None):\n",
    "\n",
    "        emissions = self.bilstm(sentence)\n",
    "        if targets is not None:\n",
    "            # 计算CRF损失\n",
    "            # mask = (sentence != 0)  # 使用 0 填充的词的位置作为掩码\n",
    "            crf_loss = self.crf(emissions, targets, mask)\n",
    "            return crf_loss\n",
    "        else:\n",
    "            # 测试时，使用维特比解码\n",
    "            tags = self.crf.viterbi_decode(emissions, mask)\n",
    "            return tags\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:36:24.586359Z",
     "start_time": "2024-06-11T12:36:24.378062Z"
    }
   },
   "source": [
    "\n",
    "train_data = get_train_data(language)\n",
    "valid_data = get_valid_data(language)\n",
    "\n",
    "import os\n",
    "import torchtext\n",
    "\n",
    "train_word = [word for sentence in train_data for word, label in sentence]\n",
    "print(len(train_word))\n",
    "\n",
    "vocab1 = torchtext.vocab.vocab(Counter(train_word), min_freq=min_freq, specials=['<unk>'])\n",
    "vocab1.set_default_index(vocab1['<unk>'])\n",
    "train_data = train_data + valid_data\n",
    "train_word = [word for sentence in train_data for word, label in sentence]\n",
    "print(len(train_word))\n",
    "\n",
    "vocab2 = torchtext.vocab.vocab(Counter(train_word), min_freq=min_freq, specials=['<unk>'])\n",
    "vocab2.set_default_index(vocab2['<unk>'])\n",
    "print(vocab2.get_default_index())\n",
    "if for_test:\n",
    "    vocab = vocab2\n",
    "else:\n",
    "    vocab = vocab1\n",
    "\n",
    "\n",
    "# vocab不变\n",
    "def sent2word(sentence):\n",
    "    return [w for w, _ in sentence]\n",
    "\n",
    "\n",
    "def sent2label(sentence):\n",
    "    return [l for _, l in sentence]\n",
    "\n",
    "\n",
    "max_length = max([len(l) for l in train_data])\n",
    "max_length = max(max_length, 256)\n",
    "sorted_labels = sorted_labels_chn if language == 'Chinese' else sorted_labels_eng\n",
    "\n",
    "\n",
    "def label2index(label):\n",
    "    return sorted_labels.index(label)\n",
    "\n",
    "\n",
    "print(len(vocab))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123967\n",
      "137849\n",
      "0\n",
      "1148\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:36:24.602275Z",
     "start_time": "2024-06-11T12:36:24.586359Z"
    }
   },
   "source": [
    "\n",
    "# Create a custom dataset\n",
    "custom_dataset = CustomDataset(train_data, vocab, label2index, max_length)\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "custom_dataset[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_embeddings': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0]),\n",
       " 'label_indices': tensor([ 0,  0, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 13, 14, 14, 14, 14, 15,\n",
       "          0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'max_length': 19}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:36:24.617913Z",
     "start_time": "2024-06-11T12:36:24.602275Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, train_loader, num_epochs, learning_rate, device):\n",
    "    # Move the model to GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create a progress bar\n",
    "    progress_bar = tqdm(total=num_epochs * len(train_loader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Move inputs, labels, and mask to GPU\n",
    "            length = batch['max_length']\n",
    "            max_length = np.argmax(length)\n",
    "            aaa = length[max_length]\n",
    "            inputs = batch['word_embeddings'][:, :aaa].to(device)\n",
    "            labels = batch['label_indices'][:, :aaa].to(device)\n",
    "            mask = batch['mask'].to(device)[:, :aaa].to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            loss = model(inputs, mask, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.set_postfix_str(\"Epoch:{}, Loss:{}\".format(epoch + 1, sum_loss / len(train_loader)))\n",
    "        print(\" \")\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a model, train_loader, num_epochs, and learning_rate\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:36:24.665313Z",
     "start_time": "2024-06-11T12:36:24.617913Z"
    }
   },
   "source": [
    "bilstm_crf = BiLSTM_CRF(len(sorted_labels), len(vocab), embed_size, hidden_dim)\n",
    "pretrain_file = \"./bilstm_crf/pretrain/BILSTM_{}.bin\".format(language)\n",
    "if for_test:\n",
    "    save_file = \"./weight/bilstm/BILSTM_CRF_{}_final__temp.bin\".format(language)\n",
    "else:\n",
    "    save_file = \"./weight/bilstm/BILSTM_CRF_{}__temp.bin\".format(language)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_file1 = \"./weight/bilstm/BILSTM_CRF_{}.bin\".format(language)\n",
    "\n",
    "load_file2 = \"./weight/bilstm/BILSTM_CRF_{}_final.bin\".format(language)\n",
    "if mode:\n",
    "    train(bilstm_crf, dataloader, 10, 1e-3, device)\n",
    "    torch.save(bilstm_crf.state_dict(), save_file)\n",
    "else:\n",
    "    if for_test:\n",
    "        load_file = load_file2\n",
    "    else:\n",
    "        load_file = load_file1\n",
    "    bilstm_crf.load_state_dict(torch.load(load_file))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:36:24.680934Z",
     "start_time": "2024-06-11T12:36:24.665313Z"
    }
   },
   "source": [
    "def mycheck(language, vocab, res_file, model, train_or_valid, device):\n",
    "    valid = get_data_from_file(res_file)\n",
    "    pred_path = \"example_data/BILSTM_CRF_{}_{}.txt\".format(\n",
    "        language, train_or_valid)\n",
    "    valid_data = CustomDataset(valid, vocab, label2index, 256)\n",
    "    valdataloader = DataLoader(valid_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Move the model to GPU\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    with open(pred_path, \"w\", encoding='utf-8') as f:\n",
    "        with torch.no_grad():\n",
    "            iter = 0\n",
    "            for batch in valdataloader:\n",
    "                # Move inputs, labels, and mask to GPU\n",
    "                length = batch['max_length']\n",
    "                max_length = np.argmax(length)\n",
    "                aaa = length[max_length]\n",
    "                word_embeddings = batch['word_embeddings'][:, :aaa]\n",
    "                masks = batch['mask'][:, :aaa]\n",
    "\n",
    "                preds = model(word_embeddings, masks)\n",
    "\n",
    "                for pred in preds:\n",
    "                    pred_labels = []\n",
    "                    for i in range(len(valid[iter])):\n",
    "                        f.write(valid[iter][i][0] + \" \" +\n",
    "                                sorted_labels[pred[i]] + '\\n')\n",
    "                        pred_labels.append(sorted_labels[pred[i]])\n",
    "                    f.write('\\n')\n",
    "                    iter = iter + 1\n",
    "\n",
    "    # Move the model back to CPU if needed\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "    res = check(language, res_file, pred_path)\n",
    "    return res\n",
    "\n",
    "\n",
    "def test(language, res_file, device):\n",
    "    bilstm_crf2 = BiLSTM_CRF(len(sorted_labels), len(\n",
    "        vocab2), embed_size, hidden_dim)\n",
    "    if mode:\n",
    "        bilstm_crf2.load_state_dict(torch.load(save_file))\n",
    "    else:\n",
    "\n",
    "        bilstm_crf2.load_state_dict(torch.load(load_file2))\n",
    "    # res1 = mycheck(language=language,vocab=vocab1,res_file=res_file,model=bilstm_crf1,train_or_valid=\"test\",device=device)\n",
    "    res2 = mycheck(language=language, vocab=vocab2, res_file=res_file,\n",
    "                   model=bilstm_crf2, train_or_valid=\"test\", device=device)\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:36:24.696597Z",
     "start_time": "2024-06-11T12:36:24.680934Z"
    }
   },
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#mycheck(language=language,vocab=vocab,res_file=\"{}/train.txt\".format(language),model=bilstm_crf,train_or_valid=\"train\",device=device)\n",
    "#mycheck(language=language,vocab=vocab,res_file=\"{}/validation.txt\".format(language),model=bilstm_crf,train_or_valid=\"valid\",device=device)\n",
    "#language train val\n",
    "#chinese 0.9912  0.9497\n",
    "#english 0.9898  0.8205"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T12:36:26.729641Z",
     "start_time": "2024-06-11T12:36:24.696597Z"
    }
   },
   "source": [
    "\n",
    "test(language=language, res_file=\"test/{}/test.txt\".format(language), device=device)\n",
    "# 0.7519 \n",
    "# 0.9461\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding for gold_path: utf-8\n",
      "Detected encoding for my_path: utf-8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-NAME     1.0000    0.9107    0.9533       112\n",
      "      M-NAME     0.9750    0.9512    0.9630        82\n",
      "      E-NAME     0.9906    0.9375    0.9633       112\n",
      "      S-NAME     0.0000    0.0000    0.0000         0\n",
      "      B-CONT     1.0000    1.0000    1.0000        28\n",
      "      M-CONT     1.0000    1.0000    1.0000        53\n",
      "      E-CONT     1.0000    1.0000    1.0000        28\n",
      "      S-CONT     0.0000    0.0000    0.0000         0\n",
      "       B-EDU     0.9735    0.9821    0.9778       112\n",
      "       M-EDU     0.9884    0.9553    0.9716       179\n",
      "       E-EDU     0.9910    0.9821    0.9865       112\n",
      "       S-EDU     0.0000    0.0000    0.0000         0\n",
      "     B-TITLE     0.9385    0.9312    0.9348       770\n",
      "     M-TITLE     0.9358    0.9099    0.9227      1921\n",
      "     E-TITLE     0.9819    0.9844    0.9831       770\n",
      "     S-TITLE     0.0000    0.0000    0.0000         0\n",
      "       B-ORG     0.9706    0.9565    0.9635       552\n",
      "       M-ORG     0.9666    0.9664    0.9665      4312\n",
      "       E-ORG     0.9422    0.9149    0.9283       552\n",
      "       S-ORG     0.0000    0.0000    0.0000         0\n",
      "      B-RACE     1.0000    0.9286    0.9630        14\n",
      "      M-RACE     0.0000    0.0000    0.0000         0\n",
      "      E-RACE     1.0000    1.0000    1.0000        14\n",
      "      S-RACE     0.0000    0.0000    0.0000         0\n",
      "       B-PRO     0.8788    0.8788    0.8788        33\n",
      "       M-PRO     0.7647    0.9559    0.8497        68\n",
      "       E-PRO     0.8889    0.9697    0.9275        33\n",
      "       S-PRO     0.0000    0.0000    0.0000         0\n",
      "       B-LOC     1.0000    0.8333    0.9091         6\n",
      "       M-LOC     1.0000    0.8095    0.8947        21\n",
      "       E-LOC     1.0000    0.8333    0.9091         6\n",
      "       S-LOC     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.9583    0.9492    0.9537      9890\n",
      "   macro avg     0.6933    0.6747    0.6827      9890\n",
      "weighted avg     0.9586    0.9492    0.9537      9890\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
